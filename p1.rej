Index: test-libhdfs.sh
--- test-libhdfs.sh	(revision 726973)
+++ test-libhdfs.sh	(working copy)
@@ -21,6 +21,8 @@
 # c) HADOOP_CONF_DIR 
 # d) HADOOP_LOG_DIR 
 # e) LIBHDFS_BUILD_DIR
+# f) LIBHDFS_INSTALL_DIR
+# g) OS_NAME
 # All these are passed by build.xml.
 #
 
@@ -59,6 +61,9 @@
   CLASSPATH=${CLASSPATH}:$f;
 done
 
+for ff in $HADOOP_HOME/*.jar; do 
+  CLASSPATH=${CLASSPATH}:$ff
+done
 for f in $HADOOP_HOME/lib/jsp-2.0/*.jar; do
   CLASSPATH=${CLASSPATH}:$f;
 done
@@ -72,14 +77,49 @@
 # restore ordinary behaviour
 unset IFS
 
+findlibjvm () {
+javabasedir=$JAVA_HOME
+case $OS_NAME in
+    cygwin* | mingw* | pw23* )
+    lib_jvm_dir=`find $javabasedir -follow \( \
+        \( -name client -type d -prune \) -o \
+        \( -name "jvm.dll" -exec dirname {} \; \) \) 2> /dev/null | tr "\n" " "`
+    ;;
+    aix*)
+    lib_jvm_dir=`find $javabasedir \( \
+        \( -name client -type d -prune \) -o \
+        \( -name "libjvm.*" -exec dirname {} \; \) \) 2> /dev/null | tr "\n" " "`
+    if test -z "$lib_jvm_dir"; then
+       lib_jvm_dir=`find $javabasedir \( \
+       \( -name client -type d -prune \) -o \
+       \( -name "libkaffevm.*" -exec dirname {} \; \) \) 2> /dev/null | tr "\n" " "`
+    fi
+    ;;
+    *)
+    lib_jvm_dir=`find $javabasedir -follow \( \
+       \( -name client -type d -prune \) -o \
+       \( -name "libjvm.*" -exec dirname {} \; \) \) 2> /dev/null | tr "\n" " "`
+    if test -z "$lib_jvm_dir"; then
+       lib_jvm_dir=`find $javabasedir -follow \( \
+       \( -name client -type d -prune \) -o \
+       \( -name "libkaffevm.*" -exec dirname {} \; \) \) 2> /dev/null | tr "\n" " "`
+    fi
+    ;;
+  esac
+  echo $lib_jvm_dir
+}
+LIB_JVM_DIR=`findlibjvm`
+echo  "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
+echo  LIB_JVM_DIR = $LIB_JVM_DIR
+echo  "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
 # Put delays to ensure hdfs is up and running and also shuts down 
 # after the tests are complete
 cd $HADOOP_HOME
 echo Y | $HADOOP_BIN_DIR/hadoop namenode -format &&
 $HADOOP_BIN_DIR/hadoop-daemon.sh start namenode && sleep 2 && 
 $HADOOP_BIN_DIR/hadoop-daemon.sh start datanode && sleep 2 && 
-echo CLASSPATH=$HADOOP_CONF_DIR:$CLASSPATH LD_PRELOAD="$LIBHDFS_BUILD_DIR/libhdfs.so" $LIBHDFS_BUILD_DIR/$HDFS_TEST && 
-CLASSPATH=$HADOOP_CONF_DIR:$CLASSPATH LD_PRELOAD="$LIBHDFS_BUILD_DIR/libhdfs.so" $LIBHDFS_BUILD_DIR/$HDFS_TEST
+echo CLASSPATH=$HADOOP_CONF_DIR:$CLASSPATH LD_PRELOAD="$LIBHDFS_INSTALL_DIR/libhdfs.so:$LIB_JVM_DIR/libjvm.so" $LIBHDFS_BUILD_DIR/$HDFS_TEST && 
+CLASSPATH=$HADOOP_CONF_DIR:$CLASSPATH LD_PRELOAD="$LIB_JVM_DIR/libjvm.so:$LIBHDFS_INSTALL_DIR/libhdfs.so:" $LIBHDFS_BUILD_DIR/$HDFS_TEST
 BUILD_STATUS=$?
 sleep 3
 $HADOOP_BIN_DIR/hadoop-daemon.sh stop datanode && sleep 2 && 
Index: Makefile
--- Makefile	(revision 726973)
+++ Makefile	(working copy)
@@ -1,78 +0,0 @@
-#
-# Copyright 2005 The Apache Software Foundation
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-#
-# Note: This makefile depends on 4 environment variables to funtion correctly:
-# a) JAVA_HOME
-# b) OS_NAME
-# c) OS_ARCH
-# d) LIBHDFS_BUILD_DIR
-# All these are passed by build.xml.
-#
-
-CC = gcc
-LD = gcc
-CFLAGS =  -g -Wall -O2 -fPIC
-LDFLAGS = -L$(JAVA_HOME)/jre/lib/$(OS_ARCH)/server -ljvm -shared -m32 -Wl,-x 
-PLATFORM = $(shell echo $$OS_NAME | tr [A-Z] [a-z])
-CPPFLAGS = -m32 -I$(JAVA_HOME)/include -I$(JAVA_HOME)/include/$(PLATFORM)
-
-LIB_NAME = hdfs
-SO_NAME = lib$(LIB_NAME).so
-SO_TARGET = $(LIBHDFS_BUILD_DIR)/$(SO_NAME).$(SHLIB_VERSION)
-SO = $(LIBHDFS_BUILD_DIR)/$(SO_NAME)
-
-RM = rm -rf
-LINK = ln -sf
-DOXYGEN = doxygen
-
-CSRC = \
-	hdfs.c \
-	hdfsJniHelper.c \
-	$(NONE)
-
-COBJS = $(addprefix $(LIBHDFS_BUILD_DIR)/,$(patsubst %,%.o,$(basename $(CSRC))))
-
-HDFS_TEST = $(LIBHDFS_BUILD_DIR)/hdfs_test
-HDFS_READ_TEST = $(LIBHDFS_BUILD_DIR)/hdfs_read
-HDFS_WRITE_TEST = $(LIBHDFS_BUILD_DIR)/hdfs_write
-
-all: $(SO_TARGET) $(HDFS_TEST) $(HDFS_READ_TEST) $(HDFS_WRITE_TEST)
-
-$(SO_TARGET): $(COBJS)
-	$(LD) $(LDFLAGS) -o $(SO_TARGET) -Wl,-soname,$(SO_NAME) $(COBJS) \
-	&& $(LINK) $(SO_TARGET) $(SO)
-	
-$(LIBHDFS_BUILD_DIR)/%.o: %.c
-	$(CC) $(CFLAGS) $(CPPFLAGS) -c $< -o $@
-
-$(HDFS_TEST): hdfs_test.c
-	$(CC) $(CPPFLAGS) $< -L$(LIBHDFS_BUILD_DIR) -l$(LIB_NAME) -o $@
-
-$(HDFS_READ_TEST): hdfs_read.c
-	$(CC) $(CPPFLAGS) $< -Wl,-rpath,. -L$(LIBHDFS_BUILD_DIR) -l$(LIB_NAME) -o $@ 
-
-$(HDFS_WRITE_TEST): hdfs_write.c
-	$(CC) $(CPPFLAGS) $< -Wl,-rpath,. -L$(LIBHDFS_BUILD_DIR) -l$(LIB_NAME) -o $@
-
-doc:
-	$(DOXYGEN) docs/Doxyfile
-
-test: $(HDFS_TEST)
-	./tests/test-libhdfs.sh	
-
-# vim: sw=4: ts=4: noet
-
Index: build.xml
--- build.xml	(revision 726973)
+++ build.xml	(working copy)
@@ -30,7 +30,6 @@
   <property name="version" value="0.20.0-dev"/>
   <property name="final.name" value="${name}-${version}"/>
   <property name="year" value="2008"/>
-  <property name="libhdfs.version" value="1"/>
 
   <property name="src.dir" value="${basedir}/src"/>  	
   <property name="core.src.dir" value="${src.dir}/core"/>
@@ -51,7 +50,7 @@
   <property name="c++.utils.src" value="${c++.src}/utils"/>
   <property name="c++.pipes.src" value="${c++.src}/pipes"/>
   <property name="c++.examples.pipes.src" value="${examples.dir}/pipes"/>
-  <property name="libhdfs.src" value="${c++.src}/libhdfs"/>
+  <property name="c++.libhdfs.src" value="${c++.src}/libhdfs"/>
   <property name="librecordio.src" value="${c++.src}/librecordio"/>
   <property name="tools.src" value="${basedir}/src/tools"/>
 
@@ -63,7 +62,6 @@
   <property name="build.webapps" value="${build.dir}/webapps"/>
   <property name="build.examples" value="${build.dir}/examples"/>
   <property name="build.anttasks" value="${build.dir}/ant"/>
-  <property name="build.libhdfs" value="${build.dir}/libhdfs"/>
   <property name="build.librecordio" value="${build.dir}/librecordio"/>
   <!-- convert spaces to _ so that mac os doesn't break things -->
   <exec executable="sed" inputstring="${os.name}" 
@@ -72,10 +70,13 @@
   </exec>
   <property name="build.platform" 
             value="${nonspace.os}-${os.arch}-${sun.arch.data.model}"/>
+  <property name="jvm.arch" 
+            value="${sun.arch.data.model}"/>
   <property name="build.native" value="${build.dir}/native/${build.platform}"/>
   <property name="build.c++" value="${build.dir}/c++-build/${build.platform}"/>
   <property name="build.c++.utils" value="${build.c++}/utils"/>
   <property name="build.c++.pipes" value="${build.c++}/pipes"/>
+  <property name="build.c++.libhdfs" value="${build.c++}/libhdfs"/>
   <property name="build.c++.examples.pipes" 
             value="${build.c++}/examples/pipes"/>
   <property name="build.docs" value="${build.dir}/docs"/>
@@ -110,8 +111,8 @@
   <property name="test.junit.haltonfailure" value="no" />
   <property name="test.junit.maxmemory" value="512m" />
 
-  <property name="libhdfs.test.conf.dir" value="${libhdfs.src}/tests/conf"/>
-  <property name="libhdfs.test.dir" value="${test.build.dir}/libhdfs"/>
+  <property name="test.libhdfs.conf.dir" value="${c++.libhdfs.src}/tests/conf"/>
+  <property name="test.libhdfs.dir" value="${test.build.dir}/libhdfs"/>
 
   <property name="librecordio.test.dir" value="${test.build.dir}/librecordio"/>
   <property name="web.src.dir" value="${basedir}/src/web"/>
@@ -504,7 +505,7 @@
   	description="Compile core only">
   </target>
 
-  <target name="compile-contrib" depends="compile-core,compile-libhdfs">
+  <target name="compile-contrib" depends="compile-core,compile-c++-libhdfs">
      <subant target="compile">
         <property name="version" value="${version}"/>
         <fileset file="${contrib.dir}/build.xml"/>
@@ -1053,7 +1054,7 @@
   <!-- ================================================================== -->
   <!--                                                                    -->
   <!-- ================================================================== -->
-  <target name="package" depends="compile, jar, javadoc, examples, tools-jar, jar-test, ant-tasks, package-libhdfs, package-librecordio"
+  <target name="package" depends="compile, jar, javadoc, examples, tools-jar, jar-test, ant-tasks, package-librecordio"
 	  description="Build distribution">
     <mkdir dir="${dist.dir}"/>
     <mkdir dir="${dist.dir}/lib"/>
@@ -1206,65 +1207,28 @@
         <fileset file="src/contrib/build.xml"/>
      </subant>  	
   </target>
-
-  <!-- ================================================================== -->
-  <!-- libhdfs targets.                                                   -->
-  <!-- ================================================================== -->
-  <target name="compile-libhdfs" depends="init" if="libhdfs">
-    <mkdir dir="${build.libhdfs}"/>
-    <exec dir="${libhdfs.src}" executable="${make.cmd}" failonerror="true">
-      <env key="OS_NAME" value="${os.name}"/>
-      <env key="OS_ARCH" value="${os.arch}"/>
-      <env key="SHLIB_VERSION" value="${libhdfs.version}"/>
-      <env key="LIBHDFS_BUILD_DIR" value="${build.libhdfs}"/>
-    </exec>
-  </target>
 	
-  <target name="test-libhdfs" depends="compile-libhdfs, compile-core">
-    <delete dir="${libhdfs.test.dir}"/>
-    <mkdir dir="${libhdfs.test.dir}"/>
-    <mkdir dir="${libhdfs.test.dir}/logs"/>
-    <mkdir dir="${libhdfs.test.dir}/hdfs/name"/>
+ <target name="test-c++-libhdfs" depends="compile-c++-libhdfs, compile-core" if="islibhdfs">
+    <delete dir="${test.libhdfs.dir}"/>
+    <mkdir dir="${test.libhdfs.dir}"/>
+    <mkdir dir="${test.libhdfs.dir}/logs"/>
+    <mkdir dir="${test.libhdfs.dir}/hdfs/name"/>
 
-    <exec dir="${libhdfs.src}" executable="${make.cmd}" failonerror="true">
+    <exec dir="${build.c++.libhdfs}" executable="${make.cmd}" failonerror="true">
         <env key="OS_NAME" value="${os.name}"/>
         <env key="OS_ARCH" value="${os.arch}"/>
-        <env key="SHLIB_VERSION" value="${libhdfs.version}"/>
-        <env key="LIBHDFS_BUILD_DIR" value="${build.libhdfs}"/>
+        <env key="JVM_ARCH" value="${jvm.arch}"/>
+        <env key="LIBHDFS_BUILD_DIR" value="${build.c++.libhdfs}"/>
         <env key="HADOOP_HOME" value="${basedir}"/>
-        <env key="HADOOP_CONF_DIR" value="${libhdfs.test.conf.dir}"/>
-        <env key="HADOOP_LOG_DIR" value="${libhdfs.test.dir}/logs"/>
+        <env key="HADOOP_CONF_DIR" value="${test.libhdfs.conf.dir}"/>
+        <env key="HADOOP_LOG_DIR" value="${test.libhdfs.dir}/logs"/>
+        <env key="LIBHDFS_SRC_DIR" value="${c++.libhdfs.src}"/>
+        <env key="LIBHDFS_INSTALL_DIR" value="${install.c++}/lib"/>  
         <env key="LIB_DIR" value="${common.ivy.lib.dir}"/>
 		<arg value="test"/>
     </exec>
   </target>
 
-  <target name="doc-libhdfs" depends="compile-libhdfs">
-	<exec dir="${libhdfs.src}" executable="${make.cmd}">
-	  <arg value="doc"/>
-	</exec>
-  </target>
-	
-  <target name="package-libhdfs"
-	  depends="compile-libhdfs, doc-libhdfs"
-	  if="libhdfs">
-  	<mkdir dir="${dist.dir}/libhdfs"/>
-  	<copy todir="${dist.dir}/libhdfs">
-  	  <fileset dir="${build.libhdfs}" 
-  	  	casesensitive="yes" 
-  	  	followsymlinks="false">
-  	  	<exclude name="**/tests/**"/>
-  	  	<exclude name="*.so"/>
-  	  	<exclude name="*.o"/>
-  	  </fileset>
-  	</copy>
-  	<chmod perm="ugo+x" type="file">
-  	    <fileset dir="${dist.dir}/libhdfs"/>
-  	</chmod>
-  	<exec dir="${dist.dir}/libhdfs" executable="ln">
-  		<arg line="-sf libhdfs.so.${libhdfs.version} libhdfs.so"/>
-  	</exec>
-  </target>
 <!-- ================================================================== -->
 <!-- librecordio targets.                                               -->
 <!-- ================================================================== -->		
@@ -1316,7 +1280,15 @@
           searchpath="yes" failonerror="yes">
        <arg value="-if"/>
     </exec>
+    <antcall target="create-c++-configure-libhdfs"/>
   </target>
+   
+  <target name="create-c++-configure-libhdfs" depends="check-c++-libhdfs" if="islibhdfs">
+    <exec executable="autoreconf" dir="${c++.libhdfs.src}" 
+          searchpath="yes" failonerror="yes">
+       <arg value="-if"/>
+    </exec>
+  </target>
 
   <target name="check-c++-makefiles" depends="init" if="compile.c++">
     <condition property="need.c++.utils.makefile">
@@ -1330,6 +1302,33 @@
     </condition>
   </target>
 
+  <target name="check-c++-libhdfs">
+    <condition property="islibhdfs">
+      <and>
+        <isset property="compile.c++"/>
+        <isset property="libhdfs"/>
+      </and>
+    </condition>
+  </target>
+
+  <target name="check-c++-makefile-libhdfs" depends="init,check-c++-libhdfs" if="islibhdfs">
+    <condition property="need.c++.libhdfs.makefile">
+       <not> <available file="${build.c++.libhdfs}/Makefile"/> </not>
+    </condition>
+  </target>
+
+  <target name="create-c++-libhdfs-makefile" depends="check-c++-makefile-libhdfs" 
+                                           if="need.c++.libhdfs.makefile">
+    <mkdir dir="${build.c++.libhdfs}"/>
+    <chmod file="${c++.libhdfs.src}/configure" perm="ugo+x"/>
+    <exec executable="${c++.libhdfs.src}/configure" dir="${build.c++.libhdfs}"
+          failonerror="yes">
+      <env key="ac_cv_func_malloc_0_nonnull" value="yes"/>
+      <env key="JVM_ARCH" value="${jvm.arch}"/>
+      <arg value="--prefix=${install.c++}"/>
+    </exec>
+  </target>
+
   <target name="create-c++-utils-makefile" depends="check-c++-makefiles" 
                                            if="need.c++.utils.makefile">
     <mkdir dir="${build.c++.utils}"/>
@@ -1393,6 +1392,17 @@
   <target name="compile-c++-examples" 
           depends="compile-c++-examples-pipes"/>
 
+  <target name="compile-c++-libhdfs" depends="create-c++-libhdfs-makefile" if="islibhdfs">
+    <exec executable="${make.cmd}" dir="${build.c++.libhdfs}" searchpath="yes"
+          failonerror="yes">
+      <env key="ac_cv_func_malloc_0_nonnull" value="yes"/>
+      <env key="JVM_ARCH" value="${jvm.arch}"/>
+      <arg value="install"/>
+    </exec>
+  </target>
+
+
+
   <target name="compile-ant-tasks" depends="compile-core">
     <javac
         encoding="${build.encoding}"
